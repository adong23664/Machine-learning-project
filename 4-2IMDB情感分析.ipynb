{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析第一件事\n",
    "\n",
    "首先，我們必須將分類數據轉換為數字形式，然後才能將其傳遞給機器學習算法。 我們使用“詞袋”模型進行此操作。 其背後的想法很簡單，可以總結如下：\n",
    "1.我們從整個文檔集中創建具有唯一**符記（例如單詞）的**詞彙**。\n",
    "2.我們從每個文檔構造一個特徵向量，其中包含每個單詞在特定文檔中出現的頻率計數。\n",
    "\n",
    "### 將單詞轉換為特徵向量\n",
    "\n",
    "要建立詞袋模型，我們可以使用在scikit-learn中實現的CountVectorizer，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#導入需要套件\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立詞袋模型\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'\n",
    "])\n",
    "\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通過tf-idf評估單詞相關性\n",
    "\n",
    "此技術可用於減輕特徵向量中不包含有用或歧視性信息的頻繁出現的單詞的量。 scikit-learn通過TfidfTransformer實現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.43 0.   0.   0.56 0.43 0.56]\n",
      " [0.4  0.48 0.31 0.31 0.31 0.48 0.31]]\n"
     ]
    }
   ],
   "source": [
    "# 計算TF/IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清理文字資料\n",
    "\n",
    "在建立詞袋模型前，以刪除所有不需要的字元來清理文字資料。 以後隨機文檔中的最後60個字元來說明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If only I could understand the right meaning of the lyrics:('"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 導入電影評論檔案\n",
    "df = pd.read_csv('movie_data.csv')\n",
    "\n",
    "df.loc[49941, 'review'][-60:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此處將刪除所有HTML標記以及標點符號和其他非字母字元，僅保留在情感分析肯定有用的表情符號字元，可以使用Python的正則表示式庫`re`完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 刪除所有HTML標記以及標點符號和其他非字母字元，並僅保留表情符號字元\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text = text + \" \".join(emoticons).replace('-', '')\n",
    "    return text\n",
    "\n",
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "49959  my first attempt at watching this ended in 8 m...          0\n",
      "49960  pepe le moko played by charles boyer is some s...          0\n",
      "49961  awful simply awful it proves my theory about s...          0\n",
      "49962  i attended camp chesapeake it was located at t...          1\n",
      "49963  i was impressed that i could take my 5 year ol...          1\n",
      "49964  this movie is terrible it s about some no brai...          0\n",
      "49965  well what was fun except for the fun part it s...          0\n",
      "49966  by the time this film was released i had seen ...          0\n",
      "49967  well if you like pop punk punk ska and a tad b...          0\n",
      "49968  where this movie is faithful to burroughs visi...          1\n"
     ]
    }
   ],
   "source": [
    "# 實際進行刪除\n",
    "df['review'] = df['review'].apply(preprocessor)\n",
    "\n",
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將文檔處理為符記\n",
    "\n",
    "現在我們已經成功地準備了資料集，我們需要考慮如何將文本拆分為單個元素。 我們可以這樣做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將文字檔分解為單詞\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一種策略是“單詞詞幹” **，這是將單詞轉換為詞根形式的過程，使我們可以將相關單詞映射到同一詞幹。 我們將使用由`nltk`軟件包實現的** Porter阻止算法**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我們開始使用詞袋對機器學習模型進行訓練之前，讓我們刪除那些不會在文本中添加有用信息的極其常見的詞（稱為“停用詞”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 訓練羅吉斯迴歸模型進行文檔分類\n",
    "\n",
    "此處將訓練羅吉斯迴歸模型將電影評論分為正面評論和負面評論。 首先，讓我們將清理後的文件檔的“ DataFrame”劃分為50:50的比例進行訓練和測試。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.review\n",
    "y = df.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來，我們將使用“ GridSearchCV”通過3折交叉驗證對我們的羅吉斯迴歸模型進行超參數調整："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 28.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x00000277C3CDB0D8>}\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, \n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "              'vect__stop_words': [stop, None],\n",
    "              'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "              'clf__penalty': ['l1', 'l2'],\n",
    "              'clf__C': [1.0, 10.0, 100.0]},\n",
    "             {'vect__ngram_range': [(1,1)],\n",
    "              'vect__stop_words': [stop, None],\n",
    "              'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "              'vect__use_idf': [False],\n",
    "              'vect__norm': [None],\n",
    "              'clf__penalty': ['l1', 'l2'],\n",
    "              'clf__C': [1.0, 10.0, 100.0]}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), \n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, \n",
    "                           scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter set: %s' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.891\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "sentiment      0      1\n",
      "row_0                  \n",
      "0          11057   1175\n",
      "1           1348  11405\n",
      "0.8990194116469882\n"
     ]
    }
   ],
   "source": [
    "# 求混淆矩陣(Confusion Matrix)，計算準確度\n",
    "print('Confusion Matrix')\n",
    "preds = gs_lr_tfidf.predict(X_test)\n",
    "print(pd.crosstab(preds, y_test))\n",
    "print(gs_lr_tfidf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 1 1 1 0]\n",
      "38954    0\n",
      "30836    0\n",
      "43372    1\n",
      "39814    1\n",
      "49826    1\n",
      "16166    0\n",
      "20583    1\n",
      "10278    1\n",
      "22531    1\n",
      "29521    0\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(gs_lr_tfidf.predict(X_test[0:10]))\n",
    "print(y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38954    this movie is such a total waste of time i can...\n",
      "30836    this was without a doubt the worst movie i hav...\n",
      "43372    when dirty dancing was on tv in the middle of ...\n",
      "39814    i don t give a movie or a show ten very often ...\n",
      "49826    though derivative labyrinth still stands as th...\n",
      "                               ...                        \n",
      "3269     this is one of the dumbest ideas for a movie r...\n",
      "20040    first of all the big named actors must need th...\n",
      "42       end of days is one of the worst big budget act...\n",
      "45973    i got a free pass to a preview of this movie l...\n",
      "32784    i don t know what the last reviewer is talking...\n",
      "Name: review, Length: 24985, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie is such a total waste of time i can t understand anyone sitting through this piece of trash oh i would have loved it when i was seven years old i think a seven year old child may have written and directed it there s no script no acting just rubbish the best acting is that by the fighting roosters i think i could whip these ninjas and i am not someone you d consider tough totally unconvincing and did not spark the least bit of interest i was yawning and laughing by the end of the first ten minutes of the film this is one that would turn people away from martial art movies great comedy bad action flick \n"
     ]
    }
   ],
   "source": [
    "print(X[38954])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
